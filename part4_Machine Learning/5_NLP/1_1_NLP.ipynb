{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53fcd1d6",
   "metadata": {},
   "source": [
    "https://www.regexpal.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1104e2d",
   "metadata": {},
   "source": [
    "### اور\n",
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b211e4d8",
   "metadata": {},
   "source": [
    "### منفی اور\n",
    "![alt text](image-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29389e54",
   "metadata": {},
   "source": [
    "### یا\n",
    "\n",
    "![alt text](image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48efcaf9",
   "metadata": {},
   "source": [
    "![alt text](image-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c878c1",
   "metadata": {},
   "source": [
    "![alt text](image-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef63f6ae",
   "metadata": {},
   "source": [
    "![alt text](image-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb2590c",
   "metadata": {},
   "source": [
    "![alt text](image-6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e62f0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install nltk\n",
    "# ! pip install hazm nltk stopwords-fa\n",
    "import nltk\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21750f03",
   "metadata": {},
   "source": [
    "![alt text](image-7.png)\n",
    "\n",
    "![alt text](image-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d673c",
   "metadata": {},
   "source": [
    "![alt text](image-9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "165aced2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'world', '!', 'This', 'is', 'a', 'test', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "EXAMPLE_TEXT = 'Hello Mr. world! This is a test sentence.'\n",
    "\n",
    "print(word_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df705570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "goose\n",
      "rokes\n",
      "python\n",
      "run\n",
      "good\n",
      "best\n"
     ]
    }
   ],
   "source": [
    "###  هد کلمه رو میاره بیرون\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('cats'))\n",
    "print(lemmatizer.lemmatize('geese'))\n",
    "print(lemmatizer.lemmatize('rokes'))\n",
    "print(lemmatizer.lemmatize('python'))\n",
    "print(lemmatizer.lemmatize('run'))\n",
    "print(lemmatizer.lemmatize('better' , pos='a'))\n",
    "print(lemmatizer.lemmatize('best' , pos='n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa79746f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'Mr.', 'world', '!', 'This', 'test', 'sentence', '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords  # حروف اضافه رو میده\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "EXAMPLE_TEXT = 'Hello Mr. world! This is a test sentence.'\n",
    "\n",
    "stop_words = set(stopwords.words('English'))\n",
    "Words = word_tokenize(EXAMPLE_TEXT)\n",
    "filter_sent=[]\n",
    "\n",
    "for w in Words:\n",
    "    if w not in stop_words:\n",
    "        filter_sent.append(w)\n",
    "filter_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50135bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
      "  'S/POS\n",
      "  (Chunk ADDRESS/NNP)\n",
      "  BEFORE/IN\n",
      "  (Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "  OF/IN\n",
      "  (Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "  OF/IN\n",
      "  (Chunk THE/NNP UNION/NNP January/NNP)\n",
      "  31/CD\n",
      "  ,/,\n",
      "  2006/CD\n",
      "  (Chunk THE/NNP PRESIDENT/NNP)\n",
      "  :/:\n",
      "  (Chunk Thank/NNP)\n",
      "  you/PRP\n",
      "  all/DT\n",
      "  ./.)\n",
      "(S\n",
      "  (Chunk Mr./NNP Speaker/NNP)\n",
      "  ,/,\n",
      "  (Chunk Vice/NNP President/NNP Cheney/NNP)\n",
      "  ,/,\n",
      "  members/NNS\n",
      "  of/IN\n",
      "  (Chunk Congress/NNP)\n",
      "  ,/,\n",
      "  members/NNS\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (Chunk Supreme/NNP Court/NNP)\n",
      "  and/CC\n",
      "  diplomatic/JJ\n",
      "  corps/NN\n",
      "  ,/,\n",
      "  distinguished/JJ\n",
      "  guests/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  fellow/JJ\n",
      "  citizens/NNS\n",
      "  :/:\n",
      "  Today/VB\n",
      "  our/PRP$\n",
      "  nation/NN\n",
      "  lost/VBD\n",
      "  a/DT\n",
      "  beloved/VBN\n",
      "  ,/,\n",
      "  graceful/JJ\n",
      "  ,/,\n",
      "  courageous/JJ\n",
      "  woman/NN\n",
      "  who/WP\n",
      "  (Chunk called/VBD America/NNP)\n",
      "  to/TO\n",
      "  its/PRP$\n",
      "  founding/NN\n",
      "  ideals/NNS\n",
      "  and/CC\n",
      "  carried/VBD\n",
      "  on/IN\n",
      "  a/DT\n",
      "  noble/JJ\n",
      "  dream/NN\n",
      "  ./.)\n",
      "(S\n",
      "  Tonight/NN\n",
      "  we/PRP\n",
      "  are/VBP\n",
      "  comforted/VBN\n",
      "  by/IN\n",
      "  the/DT\n",
      "  hope/NN\n",
      "  of/IN\n",
      "  a/DT\n",
      "  glad/JJ\n",
      "  reunion/NN\n",
      "  with/IN\n",
      "  the/DT\n",
      "  husband/NN\n",
      "  who/WP\n",
      "  was/VBD\n",
      "  taken/VBN\n",
      "  so/RB\n",
      "  long/RB\n",
      "  ago/RB\n",
      "  ,/,\n",
      "  and/CC\n",
      "  we/PRP\n",
      "  are/VBP\n",
      "  grateful/JJ\n",
      "  for/IN\n",
      "  the/DT\n",
      "  good/JJ\n",
      "  life/NN\n",
      "  of/IN\n",
      "  (Chunk Coretta/NNP Scott/NNP King/NNP)\n",
      "  ./.)\n",
      "(S (/( (Chunk Applause/NNP) ./. )/))\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import state_union  \n",
    "from nltk.tokenize import PunktSentenceTokenizer # مدل آماده برای تشخیص جمله‌ها توی متن\n",
    "import nltk\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "test_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenize = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenize.tokenize(test_text)\n",
    "\n",
    "\n",
    "def process():\n",
    "    try:\n",
    "        for i in tokenized[:4]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words) # هر کلمه با برچسب دستوری میاد مثلا اسم و قید\n",
    "            chunkgram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chinkparse = nltk.RegexpParser(chunkgram)\n",
    "            chinked = chinkparse.parse(tagged)\n",
    "            print(chinked)       # اول چاپ در ترمینال\n",
    "            chinked.draw()       # بعد نمایش گرافیکی\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf8f11b",
   "metadata": {},
   "source": [
    "![alt text](image-10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111b572f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan.n.01\n",
      "plan\n",
      "a series of steps to be carried out or goals to be accomplished\n",
      "['they drew up a six-step plan', 'they discussed plans for a new bond issue']\n",
      "{'skilful', 'practiced', 'honest', 'skillful', 'commodity', 'ripe', 'thoroughly', 'dear', 'expert', 'estimable', 'effective', 'goodness', 'secure', 'right', 'near', 'good', 'safe', 'trade_good', 'unspoilt', 'in_force', 'in_effect', 'full', 'serious', 'well', 'salutary', 'unspoiled', 'dependable', 'proficient', 'adept', 'beneficial', 'just', 'sound', 'soundly', 'undecomposed', 'respectable', 'honorable', 'upright'}\n",
      "{'ill', 'evil', 'badness', 'evilness', 'bad'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet  \n",
    "syns = wordnet.synsets('program')\n",
    "# synsets -> کلمات هم معنی\n",
    "print(syns[0].name())\n",
    "print(syns[0].lemmas()[0].name()) # \n",
    "print(syns[0].definition())\n",
    "print(syns[0].examples())\n",
    "\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for sys in wordnet.synsets('good'):\n",
    "    for i in sys.lemmas():\n",
    "        synonyms.append(i.name())\n",
    "        if i.antonyms():\n",
    "            antonyms.append(i.antonyms()[0].name())\n",
    "            \n",
    "print(set(synonyms))\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90b50eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
